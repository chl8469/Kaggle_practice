{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab_03) Ensemble.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"kyle","language":"python","name":"kyle"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7fHL3ONZ55I-"},"source":["# Week 3\n","### Context\n","\n","#### Ensemble\n","+ Voting Ensemble\n","+ Out-of-fold(OOF) Ensemble\n","+ Stacking Ensemble"]},{"cell_type":"code","metadata":{"id":"aAfXTYpj5ysB"},"source":["import os\n","from os.path import join\n","\n","import multiprocessing\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import numpy as np\n","import pandas as pd\n","\n","n_cpus = multiprocessing.cpu_count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CF50H0zF0ikl"},"source":["BASE_DIR = '../'\n","\n","train_path = join(BASE_DIR, 'data', 'MDC14', 'train.csv')\n","test_path  = join(BASE_DIR, 'data', 'MDC14', 'test.csv')\n","\n","data = pd.read_csv(train_path)\n","test = pd.read_csv(test_path)\n","\n","label = data['credit']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cbI1I-1B0ikl"},"source":["data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4yycZaLJ0ikm"},"source":["data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NE7_s4ka0ikm"},"source":["data.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJRH6rTn0ikn"},"source":["data.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OkdwcU_40ikn"},"source":["test.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMBvTX5p0iko"},"source":["test.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRksS7Rw0iko"},"source":["test.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NFa98L850iko"},"source":["# 불필요한 컬럼 제거\n","data.drop(columns=['index', 'credit'], inplace=True)\n","test.drop(columns=['index'],         inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ryG6F2i90ikp"},"source":["cat_columns = [c for c, t in zip(data.dtypes.index, data.dtypes) if t == 'O'] \n","num_columns = [c for c    in data.columns if c not in cat_columns]\n","\n","print('Categorical Columns: \\n{}\\n'.format(cat_columns))\n","print('Numeric Columns: \\n{}'.format(num_columns))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B_7JpyI_0ikp"},"source":["#### 라벨 데이터 인코딩"]},{"cell_type":"code","metadata":{"id":"v7HzAcxx0ikp"},"source":["label = label.astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dadwGR-l0ikp"},"source":["#### 전처리 프로세스 함수로 작성"]},{"cell_type":"code","metadata":{"id":"zXX8spSM0ikq"},"source":["from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","\n","def preprocess(x_train, x_valid, x_test):\n","    tmp_x_train = x_train.copy()\n","    tmp_x_valid = x_valid.copy()\n","    tmp_x_test  = x_test.copy()\n","    \n","    tmp_x_train.reset_index(drop=True, inplace=True)\n","    tmp_x_valid.reset_index(drop=True, inplace=True)\n","    \n","    # 결측치 처리\n","    \n","    # 스케일링\n","\n","    # 인코딩\n","    \n","    return tmp_x_train, tmp_x_valid, tmp_x_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U8S1qXiL0ikq"},"source":["## Ensemble\n","개인적으로 앙상블은 머신러닝의 꽃이라고 생각합니다. 단일 모델로 좋은 성능을 이끄는 것도 중요하지만, 서로 다른 모델의 다양성을 고려하여 결과를 이끌어내는 앙상블은 응용할 수 있는 방법이 매우 많습니다. <br>\n","그 중 대표적인 2가지 앙상블에 대해 실습하고 배워보도록 하겠습니다. \n","\n","### 1. Voting Ensemble\n","이름에서 알 수 있듯이 각자의 모델이 투표를 하여 클래스를 선택하는 방식의 앙상블 입니다. <br>\n","Voting 앙상블은 Sklearn 자체적으로 모델로써 지원을 하며, 사용하기도 매우 쉽습니다. <br>\n","그리고 Hard, Soft로 Voting 방식이 나뉘는데, Hard는 라벨 값으로 투표를 하는 방식이고, Soft는 확률 값을 모두 더해 가장 높은 클래스를 선택합니다.\n","\n","Voting Classifier는 Sklearn의 ensemble 패키지에 있습니다."]},{"cell_type":"code","metadata":{"id":"0Jb9nQ2p0ikq"},"source":["from sklearn.model_selection import train_test_split\n","\n","# 쪼개어진 Train, Valid 데이터의 비율은 (7:3), 내부 난수 값 42, 데이터를 쪼갤 때 섞으며 label 값으로 Stratify 하는 코드 입니다. random_state를 주석 처리하고 데이터를 확인해보시면 계속 바뀝니다.\n","x_train, x_valid, y_train, y_valid = train_test_split(data, label, \n","                                                      test_size=0.3,\n","                                                      random_state=42,\n","                                                      shuffle=True,\n","                                                      stratify=label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aGE9Fwc20ikq"},"source":["x_train, x_valid, _ = preprocess(x_train, x_valid, test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9hqCgY7r0ikq"},"source":["#### 1) 모델 불러오기 및 정의하기"]},{"cell_type":"code","metadata":{"id":"fwdf41E00ikr"},"source":["from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neural_network import MLPClassifier\n","\n","clfs = [['Logistic', LogisticRegression()],\n","        ['RandomForest', RandomForestClassifier()],\n","        ['MLP', MLPClassifier()]]\n","\n","vote_clf = VotingClassifier(clfs, voting='soft', n_jobs=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_mJUat4R0ikr"},"source":["#### 2) 모델 학습하기"]},{"cell_type":"code","metadata":{"id":"kNs9Auf90ikr"},"source":["# 여기서 x_train, y_train은 마지막 Fold\n","vote_clf.fit(x_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kYqn5Yve0ikr"},"source":["#### 3) 결과 확인하기"]},{"cell_type":"code","metadata":{"id":"H53WY0Sb0ikr"},"source":["from sklearn.metrics import f1_score, log_loss\n","\n","print('Validation F1 score : {:.4f}'.format(f1_score(y_valid, vote_clf.predict(x_valid), average='weighted')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8rJ25uqj0iks"},"source":["# voting hard 에서는 작동 x, 확률로 값을 뽑을 수 없음.\n","# print('Validation log_loss score : {:.4f}'.format(log_loss(y_valid, vote_clf.predict_proba(x_valid))))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yn5K-wTW0iks"},"source":["### 2. Out-of-fold(OOF) Ensemble (같이 푸는 실습)\n","OOF 앙상블은 KFold 교차 검증에서 생성되는 각 Fold에 대한 예측 값을 앙상블하는 기법으로 모델 검증과 함께 앙상블을 진행할 수 있다는 장점이 있습니다. <br>\n","\n","Cross Validation 파트에서 배웠던 KFold 코드를 재사용해 OOF 앙상블을 진행해보겠습니다."]},{"cell_type":"code","metadata":{"id":"NSfSNXxm0iks"},"source":["from sklearn.model_selection import StratifiedKFold\n","\n","val_scores = list()\n","oof_pred = np.zeros((#맞는 차원 집어넣기))\n","n_splits=5\n","    \n","skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n","\n","for i, (trn_idx, val_idx) in enumerate(skf.split(data, label)):\n","    x_train, y_train = data.iloc[trn_idx, :], label.iloc[trn_idx,]\n","    x_valid, y_valid = data.iloc[val_idx, :], label.iloc[val_idx,]\n","    \n","    # 전처리\n","    \n","    \n","    # 모델 정의\n","    \n","    \n","    # 모델 학습\n","\n","    \n","    # 훈련, 검증 데이터 log_loss 확인\n","    trn_logloss = \n","    val_logloss = \n","    print('{} Fold, train logloss : {:.4f}4, validation logloss : {:.4f}'.format(i, trn_logloss, val_logloss))\n","    \n","    val_scores.append(val_logloss)\n","    \n","    oof_pred += model.predict_proba(x_test) / skf.n_splits \n","\n","# 교차 검증 정확도 평균 계산하기\n","print('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dDUpOE3A0iks"},"source":["submit_path = join(BASE_DIR, 'data', 'MDC14', 'sample_submission.csv')\n","\n","submit = pd.read_csv(submit_path)\n","submit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f9bot-_M0ikt"},"source":["submit.iloc[:, 1:] = oof_pred\n","submit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-NLZRagV0ikt"},"source":["# submit.to_csv('oof_first_submit.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pz0p2elP0ikt"},"source":["## Two Stage Ensemble\n","### Stacking\n","- 지난 수업에 kFold를 활용한 OOF 앙상블에 대해 학습했습니다. Stacking은 말 그대로 모델의 결과를 쌓아서 앙상블을 하는 방식입니다.\n","- 스태킹의 원리는, 모델이 예측한 y 값은 \"실제 y 값과 매우 선형성이 높다는 점을 이용하여 y_train_pred를 변수로 사용합니다.\n","- y_train_pred는 모든 vaild fold의 예측 값을 합쳐서 만듭니다.\n","\n","1. 우선 서로 다른 모델에 대해 y_train_pred, y_test_pred 값을 모읍니다.\n","2. 모은 y_train_pred 값들을 axis=1 방향으로 합친 데이터를 new_x_train이라고 하겠습니다. \n","    - 당연히 y_test_pred 값들도 axis=1 방향으로 합쳐 new_x_test라고 합니다.\n","3. new_x_train 데이터와, y_train 데이터로 모델을 학습하고, new_x_test로 최종 y_test_pred를 예측합니다.\n","    - 이때 주로 2 stage 모델(meta model이라고도 부릅니다.)은 성능이 강력한 모델을 사용합니다. (사실 해봐야 xgb, lgb 입니다..)"]},{"cell_type":"code","metadata":{"id":"zopfh_bZ0iku"},"source":["from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","\n","val_scores = list()\n","# 결과 값들을 stacking 해야하기 때문에, (모델 개수, 샘플의 수, 3) 라는 차원으로 구성됩니다.\n","oof_train = np.zeros((6, data.shape[0], 3))\n","oof_pred  = np.zeros((6, test.shape[0], 3))\n","\n","for i, (trn_idx, val_idx) in enumerate(skf.split(data, label)):\n","    x_train, y_train = data.iloc[trn_idx, :], label.iloc[trn_idx,]\n","    x_valid, y_valid = data.iloc[val_idx, :], label.iloc[val_idx,]\n","    \n","    # 전처리\n","    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n","    \n","    # 모델 정의\n","    models = [RandomForestClassifier(n_estimators=200, max_depth=5,  random_state=42, n_jobs=(n_cpus-1)),\n","              RandomForestClassifier(n_estimators=150, max_depth=8, random_state=42, n_jobs=(n_cpus-1)),\n","              XGBClassifier(n_estimators=200, max_depth=5, random_state=42, n_jobs=(n_cpus-1)),\n","              XGBClassifier(n_estimators=150, max_depth=8, random_state=42, n_jobs=(n_cpus-1)),\n","              LGBMClassifier(n_estimators=200, max_depth=5, random_state=42, n_jobs=(n_cpus-1)),\n","              LGBMClassifier(n_estimators=150, max_depth=8, random_state=42, n_jobs=(n_cpus-1))]\n","            \n","    for j, model in enumerate(models):\n","        # 모델 학습\n","        model.fit(x_train, y_train)\n","\n","        # j번째 칸에 맞는 결과 담기.\n","        oof_train[j, val_idx,] += model.predict_proba(x_valid)\n","        oof_pred[j, :,]        += model.predict_proba(x_test) / n_splits\n","    \n","    print(f'{i} Fold, ...')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DRDSajnI0iku"},"source":["oof_train.T.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EUX9_99l0iku"},"source":["# 모은 train, test의 예측 값을 new_x_train, new_x_test로 사용합니다.\n","new_train = pd.DataFrame(np.concatenate(oof_train.T, axis=1))\n","new_test  = pd.DataFrame(np.concatenate(oof_pred.T, axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QnBfTWg90ikv"},"source":["new_train.shape, new_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"br9fIkQ70ikv"},"source":["#### OOF 앙상블을 진행합니다."]},{"cell_type":"code","metadata":{"id":"MRaPGP_F0ikv"},"source":["from sklearn.preprocessing import StandardScaler\n","\n","val_scores = list()\n","oof_pred  = np.zeros((test.shape[0], 3))\n","\n","for i, (trn_idx, val_idx) in enumerate(skf.split(new_train, label)):\n","    x_train, y_train = new_train.iloc[trn_idx, :], label[trn_idx]\n","    x_valid, y_valid = new_train.iloc[val_idx, :], label[val_idx]\n","    \n","    scaler = StandardScaler()\n","    scaler.fit(x_train)\n","    x_train     = scaler.transform(x_train)\n","    x_valid     = scaler.transform(x_valid)\n","    new_x_test  = scaler.transform(new_test)\n","\n","    # 모델 정의\n","    model = XGBClassifier(random_state=42, n_jobs=(n_cpus-1))\n","    \n","    # 모델 학습\n","    model.fit(x_train, y_train)\n","\n","    # 훈련, 검증 데이터 log_loss 확인\n","    trn_logloss = log_loss(y_train, model.predict_proba(x_train))\n","    val_logloss = log_loss(y_valid, model.predict_proba(x_valid))\n","    print('{} Fold, train logloss : {:.4f}4, validation logloss : {:.4f}'.format(i, trn_logloss, val_logloss))\n","    \n","    val_scores.append(val_logloss)\n","    \n","    # 반드시 log의 역함수인 exp를 취해주세요.\n","    oof_pred += model.predict_proba(new_x_test) / n_splits\n","    \n","\n","# 교차 검증 log loss 평균 계산하기\n","print('Cross Validation Score : {:.5f}'.format(np.mean(val_scores)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rqwODIRB0ikv"},"source":["submit.iloc[:, 1:] = oof_pred\n","submit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PJ6whp3V0ikv"},"source":["# submit.to_csv('stacking_first_submit.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"773h7zBS0ikw"},"source":["## 실습 솔루션"]},{"cell_type":"markdown","metadata":{"id":"-WVORKqk0ikw"},"source":["### 전처리 함수 만들기"]},{"cell_type":"code","metadata":{"id":"2Lt5C3ye0ikw"},"source":["from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","\n","def preprocess(x_train, x_valid, x_test):\n","    tmp_x_train = x_train.copy()\n","    tmp_x_valid = x_valid.copy()\n","    tmp_x_test  = x_test.copy()\n","    \n","    tmp_x_train.reset_index(drop=True, inplace=True)\n","    tmp_x_valid.reset_index(drop=True, inplace=True)\n","    \n","    # 결측치 처리\n","    imputer = SimpleImputer(strategy='most_frequent')\n","    tmp_x_train[cat_columns] = imputer.fit_transform(tmp_x_train[cat_columns])\n","    tmp_x_valid[cat_columns] = imputer.transform(tmp_x_valid[cat_columns])\n","    tmp_x_test[cat_columns]  = imputer.transform(tmp_x_test[cat_columns])\n","    \n","    # 스케일링\n","    scaler = StandardScaler()\n","    tmp_x_train[num_columns] = scaler.fit_transform(tmp_x_train[num_columns])\n","    tmp_x_valid[num_columns] = scaler.transform(tmp_x_valid[num_columns])\n","    tmp_x_test[num_columns]  = scaler.transform(tmp_x_test[num_columns])\n","\n","    # 인코딩\n","    ohe = OneHotEncoder(sparse=False)\n","    ohe.fit(tmp_x_train[cat_columns])\n","    \n","    tmp_x_train_cat = pd.DataFrame(ohe.transform(tmp_x_train[cat_columns]))\n","    tmp_x_valid_cat = pd.DataFrame(ohe.transform(tmp_x_valid[cat_columns]))\n","    tmp_x_test_cat  = pd.DataFrame(ohe.transform(tmp_x_test[cat_columns]))\n","    \n","    tmp_x_train.drop(columns=cat_columns, inplace=True)\n","    tmp_x_valid.drop(columns=cat_columns, inplace=True)\n","    tmp_x_test.drop(columns=cat_columns, inplace=True)\n","    \n","    tmp_x_train = pd.concat([tmp_x_train, tmp_x_train_cat], axis=1)\n","    tmp_x_valid = pd.concat([tmp_x_valid, tmp_x_valid_cat], axis=1)\n","    tmp_x_test  = pd.concat([tmp_x_test, tmp_x_test_cat], axis=1)\n","    \n","    return tmp_x_train, tmp_x_valid, tmp_x_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mg2v_H6p0ikw"},"source":["### Out-of-fold ensemble 실습 "]},{"cell_type":"code","metadata":{"id":"dJ_dZe6A0ikx"},"source":["from sklearn.model_selection import StratifiedKFold\n","\n","val_scores = list()\n","oof_pred = np.zeros((test.shape[0], 3))\n","n_splits = 5\n","\n","skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n","\n","for i, (trn_idx, val_idx) in enumerate(skf.split(data, label)):\n","    x_train, y_train = data.iloc[trn_idx, :], label.iloc[trn_idx,]\n","    x_valid, y_valid = data.iloc[val_idx, :], label.iloc[val_idx,]\n","    \n","    # 전처리\n","    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n","    \n","    # 모델 정의\n","    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n","    \n","    # 모델 학습\n","    model.fit(x_train, y_train)\n","\n","    # 훈련, 검증 데이터 log_loss 확인\n","    trn_logloss = log_loss(y_train, model.predict_proba(x_train))\n","    val_logloss = log_loss(y_valid, model.predict_proba(x_valid))\n","    print('{} Fold, train logloss : {:.4f}4, validation logloss : {:.4f}'.format(i, trn_logloss, val_logloss))\n","    \n","    val_scores.append(val_logloss)\n","    \n","    oof_pred += model.predict_proba(x_test) / skf.n_splits \n","\n","# 교차 검증 정확도 평균 계산하기\n","print('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))"],"execution_count":null,"outputs":[]}]}